## NYC TLC Taxi Data End-to-End ETL Pipeline Tutorial

### Why I Made This Repository

In early 2023, YouTube personality and data influencer [Darshil Parmar](https://www.youtube.com/@DarshilParmar) uploaded a concise tutorial video entitled [ðŸš– Uber Data Analytics | End-To-End Data Engineering Project](https://www.youtube.com/watch?v=WpQECq5Hx9g) in which he creates an ETL pipeline from scratch using yellow cab trip record data obtained from New York City's [Taxi & Limousine Commission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). 

As a data professional who has benefitted immensely from tutorials of this kind in the past, my curosity was piqued by the brevity of his video, the reason for which became clear within the first half hour: Although Darshil's choice of project structure and overall approach to the subject matter are fine, his rapid progression through steps which can be a bit delicate and error-prone for beginners seems to have left many viewers confused and frustrated at key steps in the pipeline, halting their progress through the tutorial. Less problematic but still important from an educational standpoint were certain choices made throughout the tutorial which seemed questionable to me, particularly Darshil's highly idiosyncratic surplus of dimension tables in his ERD; his use of attributes which even at the time of publication of the video were not accessible via the data source (the NYC TLC stopped providing precise location data in the form of latitude and longitude years ago); and his use of Google Cloud VMs to access Mage.ai, his success with which I was only able to reproduce after completely replacing Darshil's sequence of commands with my own (Mage is much more easily accessed through Docker).

This repository contains a reworked version of the pipeline project as presented by Darshil (with the exceptions noted above) which I was able to reproducibly execute end-to-end on multiple Windows devices. In particular, note the greatly simplified schema contained in my [star schema diagram](https://github.com/neophyte577/nyc-taxi-etl-pipeline/blob/main/erd.png). I believe this approach conveys the essence of data schema diagramming just as well as Darshil's without needlessly complicating the data structure, and is a bit closer to something one would encounter in practice. Even if one is just trying to get an idea across to a beginner, I see no reason why we shouldn't just teach it the right way the first time.

On the other hand, when it comes to location data, this version of the project is complicated somewhat by the fact that the TLC appears to have replaced their coordinate data with a distinct location ID system some years ago. Accordingly, I incorporated the TLC's Taxi Zone Lookup table in the raw data for inclusion alongside the primary dataset in the ingestion step. 

It seemed in the most recent comments that the extremely buggy implementation of Mage.ai in Google Cloud per the tutorial has given folks the most trouble by far. I included a couple alternative approaches which worked fine in my GC VMs, albeit with much higher performance specifications that the ones recommended by Darshil. However, it can't be stressed enough that if you have sufficient compute (which isn't too onerous a condition, I have no idea why Google Cloud's virtual machines have so much difficulty running Mage), it would really be best to simply run Mage out of Docker. To this end, I'll be adding another folder to this repo containing instructions on how to do so along with the necessary code.

Finally, most bafflingly to me, Darshil begins the project with a fairly large csv file of taxi trip data which he claims to have downloaded directly from the NYC TLC's website, which switched over to the parquet file format in mid-2022, roughly a year before the video was published. I can't account for this discrepancy, but I can imagine it would be frustrating for a learner to encounter an inconsistency like this so early in a tutorial aimed at beginners. Accordingly, I've reworked the tutorial repo starting with a fresh parquet file downloaded from the TLC's [Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) page, and included a short script entitled [parquet_converter.py](https://github.com/neophyte577/nyc-taxi-etl-pipeline/blob/main/parquet_converter.py), the use of which should be self-explanatory.

Needless to say, all credit for this repo goes to Darshil for all the hard work he put into his video. If you haven't already, please give his a video a like and subscribe to his channel (https://www.youtube.com/@DarshilParmar).

### How to Use This Repository

For the time being, simply follow along with Darshil's otherwise excellent tutorial video and try substituting my code for his wherever there's a discrepancy. In particular, try implementing my simpler data schema, select a virtual machine with much better specs (just don't forget to delete the instance after you're done!) than he does, and use the shell commands in the mage/ directory to access Mage. I have given the user of this repo the option of either deploying Mage in a virtual environment or directly from the root directory of the VM. Although the latter is almost never recommended, the stakes aren't very high in this context, as you'll just be deleting the VM instance once you've finished this brief tutorial; and in any case, you should ideally be running Mage in Docker anyway.

You'll notice that my Looker dashboard looks very different from Darshil's. This is due to the aforementioned lack of precise location data in the dataset, as well as the fact that Darshil spends very little of the running time exploring Looker's functionality, leaving it the viewer to generate their own dashboard. Accordingly, I attempted to follow along as much as possible and wrap up the dashboard as quickly as possible. I may return to this repo and flesh out the dashboard a bit more in anticipation of making a brief video of my own.

Final note: Some in the tech community insist that it's important for beginners to struggle with errors and bugs, and to search for solutions or work them out themselves. While there is definitely something to this view, I think it's just as important for educators to minimize the occurrence of errors which produce undue frustration or threaten to prevent further learning. This repo is intended solely to resolve the latter issue.


